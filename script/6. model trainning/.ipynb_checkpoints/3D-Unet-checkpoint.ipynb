{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4db147c-13f5-4262-a9b4-df10d679e39f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "2.10.0\n",
      "2.10.0\n",
      "GPU device not found. TensorFlow is using CPU.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "# Make sure the GPU is available.\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "if device_name == '':\n",
    "    print('GPU device not found. TensorFlow is using CPU.')\n",
    "else:\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "\n",
    "print(tf.test.is_gpu_available())\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "import segmentation_models_3D as sm\n",
    "from skimage import io\n",
    "from patchify import patchify, unpatchify\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "utils_path = os.path.abspath(os.path.join('..', 'utils'))\n",
    "\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "import file_utilities as fu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17570d94-622f-40a7-a5a3-1e99ca85d114",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "os module is part of Python standard library. Python version: 3.9.19 (main, Mar 21 2024, 17:21:27) [MSC v.1916 64 bit (AMD64)]\n",
      "absl-py==2.1.0\n",
      "aggdraw==1.3.18.post0\n",
      "alabaster==0.7.16\n",
      "annotated-types==0.6.0\n",
      "anyio==4.2.0\n",
      "app-model==0.2.6\n",
      "appdirs==1.4.4\n",
      "archspec==0.2.3\n",
      "argon2-cffi-bindings==21.2.0\n",
      "argon2-cffi==21.3.0\n",
      "asttokens==2.0.5\n",
      "astunparse==1.6.3\n",
      "async-lru==2.0.4\n",
      "attrs==23.1.0\n",
      "babel==2.11.0\n",
      "backcall==0.2.0\n",
      "beautifulsoup4==4.12.2\n",
      "bleach==4.1.0\n",
      "boltons==24.0.0\n",
      "brotli==1.0.9\n",
      "build==1.2.1\n",
      "cachetools==5.3.3\n",
      "cachey==0.2.1\n",
      "certifi==2024.2.2\n",
      "cffi==1.16.0\n",
      "chardet==5.2.0\n",
      "charset-normalizer==2.0.4\n",
      "clang==5.0\n",
      "classification-models-3d==1.0.8\n",
      "click==8.1.7\n",
      "cloudpickle==3.0.0\n",
      "colorama==0.4.6\n",
      "comm==0.2.1\n",
      "conda-libmamba-solver==24.1.0\n",
      "conda-package-handling==2.2.0\n",
      "conda-package-streaming==0.9.0\n",
      "conda==24.5.0\n",
      "contourpy==1.2.1\n",
      "cycler==0.12.1\n",
      "dask==2024.4.2\n",
      "debugpy==1.6.7\n",
      "decorator==5.1.1\n",
      "defusedxml==0.7.1\n",
      "distro==1.9.0\n",
      "docstring-parser==0.16\n",
      "docutils==0.17.1\n",
      "efficientnet-3d==1.0.1\n",
      "exceptiongroup==1.2.0\n",
      "executing==0.8.3\n",
      "fastjsonschema==2.16.2\n",
      "flatbuffers==24.3.25\n",
      "fonttools==4.51.0\n",
      "freetype-py==2.4.0\n",
      "frozendict==2.4.4\n",
      "fsspec==2024.3.1\n",
      "gast==0.4.0\n",
      "google-auth-oauthlib==0.4.6\n",
      "google-auth==2.29.0\n",
      "google-pasta==0.2.0\n",
      "grpcio==1.62.1\n",
      "h5py==3.1.0\n",
      "heapdict==1.0.1\n",
      "hsluv==5.0.4\n",
      "idna==3.4\n",
      "imageio==2.34.0\n",
      "imagesize==1.4.1\n",
      "importlib-metadata==7.1.0\n",
      "importlib-resources==6.4.0\n",
      "in-n-out==0.2.1\n",
      "ipykernel==6.25.0\n",
      "ipython==8.15.0\n",
      "jedi==0.18.1\n",
      "jinja2==3.1.3\n",
      "joblib==1.4.0\n",
      "json5==0.9.6\n",
      "jsonpatch==1.33\n",
      "jsonpointer==2.4\n",
      "jsonschema-specifications==2023.7.1\n",
      "jsonschema==4.19.2\n",
      "jupyter-client==8.6.0\n",
      "jupyter-core==5.5.0\n",
      "jupyter-events==0.8.0\n",
      "jupyter-lsp==2.2.0\n",
      "jupyter-server-terminals==0.4.4\n",
      "jupyter-server==2.10.0\n",
      "jupyterlab-pygments==0.1.2\n",
      "jupyterlab-server==2.25.1\n",
      "jupyterlab==4.0.11\n",
      "keras-applications==1.0.8\n",
      "keras-preprocessing==1.1.2\n",
      "keras==2.10.0\n",
      "kiwisolver==1.4.5\n",
      "lazy-loader==0.4\n",
      "libclang==18.1.1\n",
      "libmambapy==1.5.8\n",
      "locket==1.0.0\n",
      "magicgui==0.8.2\n",
      "mamba==1.5.8\n",
      "markdown-it-py==3.0.0\n",
      "markdown==3.6\n",
      "markupsafe==2.1.5\n",
      "matplotlib-inline==0.1.6\n",
      "matplotlib==3.8.4\n",
      "mdurl==0.1.2\n",
      "menuinst==2.0.2\n",
      "mistune==2.0.4\n",
      "munkres==1.1.4\n",
      "napari-console==0.0.9\n",
      "napari-plugin-engine==0.2.0\n",
      "napari-svg==0.1.10\n",
      "napari==0.4.19.post1\n",
      "nbclient==0.8.0\n",
      "nbconvert==7.10.0\n",
      "nbformat==5.9.2\n",
      "nest-asyncio==1.6.0\n",
      "networkx==3.2.1\n",
      "neuralplot==0.0.8\n",
      "notebook-shim==0.2.3\n",
      "notebook==7.0.8\n",
      "npe2==0.7.5\n",
      "numpy==1.26.4\n",
      "numpydoc==1.5.0\n",
      "oauthlib==3.2.2\n",
      "opt-einsum==3.3.0\n",
      "overrides==7.4.0\n",
      "packaging==23.2\n",
      "pandas==2.2.2\n",
      "pandocfilters==1.5.0\n",
      "parso==0.8.3\n",
      "partd==1.4.1\n",
      "patchify==0.2.2\n",
      "pickleshare==0.7.5\n",
      "pillow==10.3.0\n",
      "pint==0.23\n",
      "pip==23.3.1\n",
      "platformdirs==3.10.0\n",
      "pluggy==1.5.0\n",
      "ply==3.11\n",
      "pooch==1.8.1\n",
      "prometheus-client==0.14.1\n",
      "prompt-toolkit==3.0.43\n",
      "protobuf==3.19.6\n",
      "psutil==5.9.0\n",
      "psygnal==0.11.0\n",
      "pure-eval==0.2.2\n",
      "pyasn1-modules==0.4.0\n",
      "pyasn1==0.6.0\n",
      "pyconify==0.1.6\n",
      "pycosat==0.6.6\n",
      "pycparser==2.21\n",
      "pydantic-compat==0.1.2\n",
      "pydantic-core==2.18.1\n",
      "pydantic==2.7.0\n",
      "pygments==2.15.1\n",
      "pyopengl==3.1.7\n",
      "pyparsing==3.1.2\n",
      "pyproject-hooks==1.0.0\n",
      "pyqt5-qt5==5.15.2\n",
      "pyqt5-sip==12.12.2\n",
      "pyqt5==5.15.10\n",
      "pyside2==5.15.2.1\n",
      "pysocks==1.7.1\n",
      "python-dateutil==2.8.2\n",
      "python-json-logger==2.0.7\n",
      "pytz==2023.3.post1\n",
      "pywavelets==1.6.0\n",
      "pywin32==305.1\n",
      "pywinpty==2.0.10\n",
      "pyyaml==6.0.1\n",
      "pyzmq==25.1.2\n",
      "qtconsole==5.5.1\n",
      "qtpy==2.4.1\n",
      "referencing==0.30.2\n",
      "requests-oauthlib==2.0.0\n",
      "requests==2.31.0\n",
      "rfc3339-validator==0.1.4\n",
      "rfc3986-validator==0.1.1\n",
      "rich==13.7.1\n",
      "rpds-py==0.10.6\n",
      "rsa==4.9\n",
      "ruamel.yaml.clib==0.2.8\n",
      "ruamel.yaml==0.18.6\n",
      "scikit-image==0.22.0\n",
      "scikit-learn==1.4.2\n",
      "scipy==1.13.0\n",
      "segmentation-models-3d==1.0.1\n",
      "send2trash==1.8.2\n",
      "setuptools==68.2.2\n",
      "shellingham==1.5.4\n",
      "shiboken2==5.15.2.1\n",
      "sip==6.7.12\n",
      "six==1.15.0\n",
      "sniffio==1.3.0\n",
      "snowballstemmer==2.2.0\n",
      "soupsieve==2.5\n",
      "sphinx==4.5.0\n",
      "sphinxcontrib-applehelp==1.0.8\n",
      "sphinxcontrib-devhelp==1.0.6\n",
      "sphinxcontrib-htmlhelp==2.0.5\n",
      "sphinxcontrib-jsmath==1.0.1\n",
      "sphinxcontrib-qthelp==1.0.7\n",
      "sphinxcontrib-serializinghtml==1.1.10\n",
      "stack-data==0.2.0\n",
      "superqt==0.6.3\n",
      "tabulate==0.9.0\n",
      "tensorboard-data-server==0.6.1\n",
      "tensorboard-plugin-wit==1.8.1\n",
      "tensorboard==2.10.1\n",
      "tensorflow-estimator==2.10.0\n",
      "tensorflow-io-gcs-filesystem==0.31.0\n",
      "tensorflow==2.10.0\n",
      "termcolor==1.1.0\n",
      "terminado==0.17.1\n",
      "tf-estimator-nightly==2.8.0.dev2021122109\n",
      "threadpoolctl==3.4.0\n",
      "tifffile==2024.2.12\n",
      "tinycss2==1.2.1\n",
      "toml==0.10.2\n",
      "tomli-w==1.0.0\n",
      "tomli==2.0.1\n",
      "toolz==0.12.1\n",
      "tornado==6.3.3\n",
      "tqdm==4.66.2\n",
      "traitlets==5.7.1\n",
      "typer==0.12.3\n",
      "typing-extensions==4.11.0\n",
      "tzdata==2024.1\n",
      "unicodedata2==15.1.0\n",
      "urllib3==2.1.0\n",
      "vispy==0.14.2\n",
      "visualkeras==0.0.2\n",
      "wcwidth==0.2.5\n",
      "webencodings==0.5.1\n",
      "websocket-client==0.58.0\n",
      "werkzeug==3.0.2\n",
      "wheel==0.41.2\n",
      "win-inet-pton==1.1.0\n",
      "wrapt==1.12.1\n",
      "zipp==3.18.1\n",
      "zstandard==0.22.0\n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "def get_installed_packages_versions():\n",
    "    installed_packages = pkg_resources.working_set\n",
    "    packages_list = sorted([\"%s==%s\" % (i.key, i.version) for i in installed_packages])\n",
    "    return packages_list\n",
    "print(\"os module is part of Python standard library. Python version:\", sys.version)\n",
    "packages_versions = get_installed_packages_versions()\n",
    "for package in packages_versions:\n",
    "    print(package)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7136fd6d-6b26-443e-957a-eebac574ef46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# If you have 16GB GPU, try this\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n",
    "\n",
    "def Unet3D(input_shape, n_classes, conv_size=(4, 4, 4), dropout_rate_1=0.1,dropout_rate_2=0.2,dropout_rate_3=0.3,dropout_rate_4=0.5):\n",
    "    x = Input(input_shape)\n",
    "\n",
    "    conv1 = Conv3D(64, conv_size, padding='same', name = 'block1_conv1')(x)\n",
    "    conv1 = BatchNormalization(name = 'block1_bn1')(conv1)   #Not in the original network. \n",
    "    conv1 = Activation(\"relu\", name = 'block1_relu1')(conv1)\n",
    "    \n",
    "    conv1 = Conv3D(64, conv_size, padding='same', name = 'block1_conv2')(conv1)\n",
    "    conv1 = BatchNormalization(name = 'block1_bn2')(conv1)   #Not in the original network. \n",
    "    conv1 = Activation(\"relu\", name = 'block1_relu2')(conv1)\n",
    "    \n",
    "    drop1 = Dropout(dropout_rate_1, name = 'block1_drop')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block1_pool')(drop1)\n",
    "\n",
    "    conv2 = Conv3D(128, conv_size, padding='same', name = 'block2_conv1')(pool1)\n",
    "    conv2 = BatchNormalization(name = 'block2_bn1')(conv2)   #Not in the original network. \n",
    "    conv2 = Activation(\"relu\",  name = 'block2_relu1')(conv2)\n",
    "    conv2 = Conv3D(128,conv_size, padding='same',  name = 'block2_conv2')(conv2)\n",
    "    conv2 = BatchNormalization( name = 'block2_bn2')(conv2)   #Not in the original network. \n",
    "    conv2 = Activation(\"relu\",  name = 'block2_relu2')(conv2)\n",
    "    drop2 = Dropout(dropout_rate_1, name = 'block2_drop')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2),  name = 'block2_pool')(drop2)\n",
    "\n",
    "    conv3 = Conv3D(256, conv_size, padding='same', name = 'block3_conv1')(pool2)\n",
    "    conv3 = BatchNormalization(name = 'block3_bn1')(conv3)   #Not in the original network. \n",
    "    conv3 = Activation(\"relu\", name = 'block3_relu1')(conv3)\n",
    "    conv3 = Conv3D(256, conv_size, padding='same', name = 'block3_conv2')(conv3)\n",
    "    conv3 = BatchNormalization( name = 'block3_bn2')(conv3)   #Not in the original network. \n",
    "    conv3 = Activation(\"relu\", name = 'block3_relu2')(conv3)\n",
    "    drop3 = Dropout(dropout_rate_2, name = 'block3_drop')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block3_pool')(drop3)\n",
    "\n",
    "    conv4 = Conv3D(512, conv_size, padding='same', name = 'block4_conv1')(pool3)\n",
    "    conv4 = BatchNormalization( name = 'block4_bn1')(conv4)   #Not in the original network. \n",
    "    conv4 = Activation(\"relu\", name = 'block4_relu1')(conv4)\n",
    "    conv4 = Conv3D(512, conv_size, padding='same', name = 'block4_conv2')(conv4)\n",
    "    conv4 = BatchNormalization(name = 'block4_bn2')(conv4)   #Not in the original network. \n",
    "    conv4 = Activation(\"relu\", name = 'block4_relu2')(conv4)\n",
    "    drop4 = Dropout(dropout_rate_2, name = 'block4_drop')(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block4_pool')(drop4)\n",
    "\n",
    "    conv5 = Conv3D(1024, conv_size, padding='same', name = 'block5_conv1')(pool4)\n",
    "    conv5 = BatchNormalization(name = 'block5_bn1')(conv5)   #Not in the original network. \n",
    "    conv5 = Activation(\"relu\", name = 'block5_relu1')(conv5)\n",
    "    conv5 = Conv3D(1024, conv_size, padding='same', name = 'block5_conv2')(conv5)\n",
    "    conv5 = BatchNormalization(name = 'block5_bn2')(conv5)   #Not in the original network. \n",
    "    conv5 = Activation(\"relu\", name = 'block5_relu2')(conv5)\n",
    "    drop5 = Dropout(dropout_rate_3, name = 'block5_drop')(conv5)\n",
    "    pool5 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block5_pool')(drop5)\n",
    "\n",
    "    # middle\n",
    "    conv6 = Conv3D(2048, conv_size, padding='same', name ='center_block1_conv' )(pool5)\n",
    "    conv6 = BatchNormalization(name ='center_block1_bn')(conv6)   #Not in the original network. \n",
    "    conv6 = Activation(\"relu\", name ='center_block1_relu')(conv6)\n",
    "    conv6 = Conv3D(2048, conv_size, padding='same', name ='center_block2_conv')(conv6)\n",
    "    conv6 = BatchNormalization(name ='center_block2_bn')(conv6)   #Not in the original network. \n",
    "    conv6 = Activation(\"relu\", name ='center_block2_relu')(conv6)\n",
    "    drop6 = Dropout(dropout_rate_4, name = 'center_block_drop')(conv6)\n",
    "\n",
    "\n",
    "    up7 = Conv3DTranspose(2048, (2, 2, 2), strides= (2, 2, 2), padding=\"same\",  name = 'decoder_stage0_upsampling')(drop6)\n",
    "    #up7 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage0_upsampling')(drop6)\n",
    "    merge7 = concatenate([up7, drop5], name='decoder_stage0_concat', axis=4)\n",
    "    \n",
    "    conv7 = Conv3D(1024,  conv_size, padding='same', name = 'decoder_stage0a_conv')(merge7)\n",
    "    conv7 = BatchNormalization(name = 'decoder_stage0a_bn')(conv7)   #Not in the original network. \n",
    "    conv7 = Activation(\"relu\", name = 'decoder_stage0a_relu')(conv7)\n",
    "    conv7 = Conv3D(1024,  conv_size, padding='same', name = 'decoder_stage0b_conv')(conv7)\n",
    "    conv7 = BatchNormalization(name = 'decoder_stage0b_bn')(conv7)   #Not in the original network. \n",
    "    conv7 = Activation(\"relu\", name = 'decoder_stage0b_relu')(conv7)\n",
    "    drop7 = Dropout(dropout_rate_3, name = 'block7_drop')(conv7)\n",
    "\n",
    "    up8 = Conv3DTranspose(1024, (2, 2, 2), strides= (2, 2, 2), padding=\"same\")(drop7)\n",
    "    #up8 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage1_upsampling')(conv7)\n",
    "    merge8 = concatenate([up8, drop4], name='decoder_stage1_concat', axis=4)\n",
    "    \n",
    "    conv8 = Conv3D(512,  conv_size, padding='same', name = 'decoder_stage1a_conv')(merge8)\n",
    "    conv8 = BatchNormalization(name = 'decoder_stage1a_bn')(conv8)   #Not in the original network. \n",
    "    conv8 = Activation(\"relu\", name = 'decoder_stage1a_relu')(conv8)\n",
    "    conv8 = Conv3D(512, conv_size, padding='same', name = 'decoder_stage1b_conv')(conv8)\n",
    "    conv8 = BatchNormalization(name = 'decoder_stage1b_bn')(conv8)   #Not in the original network. \n",
    "    conv8 = Activation(\"relu\", name = 'decoder_stage1b_relu')(conv8)\n",
    "    drop8 = Dropout(dropout_rate_2, name = 'block8_drop')(conv8)\n",
    "    \n",
    "    up9 = Conv3DTranspose(512, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop8)\n",
    "    #up9 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage2_upsampling')(conv8)\n",
    "    merge9 = concatenate([up9, drop3], name='decoder_stage2_concat', axis=4)\n",
    "\n",
    "    conv9 = Conv3D(256,  conv_size, padding='same', name = 'decoder_stage2a_conv')(merge9)\n",
    "    conv9 = BatchNormalization(name = 'decoder_stage2a_bn')(conv9)   #Not in the original network. \n",
    "    conv9 = Activation(\"relu\", name = 'decoder_stage2a_relu')(conv9)\n",
    "    conv9 = Conv3D(256,  conv_size, padding='same', name = 'decoder_stage2b_conv')(conv9)\n",
    "    conv9 = BatchNormalization(name = 'decoder_stage2b_bn')(conv9)   #Not in the original network. \n",
    "    conv9 = Activation(\"relu\", name = 'decoder_stage2b_relu')(conv9)\n",
    "    drop9 = Dropout(dropout_rate_2, name = 'block9_drop')(conv9)\n",
    "    \n",
    "    up10 = Conv3DTranspose(256, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop9)\n",
    "    #up10 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage3_upsampling')(conv9)\n",
    "    merge10 = concatenate([up10, drop2], name='decoder_stage3_concat', axis=4)\n",
    "\n",
    "    conv10 = Conv3D(128,conv_size, padding='same', name = 'decoder_stage3a_conv')(merge10)\n",
    "    conv10 = BatchNormalization(name = 'decoder_stage3a_bn')(conv10)   #Not in the original network. \n",
    "    conv10 = Activation(\"relu\", name = 'decoder_stage3a_relu')(conv10)\n",
    "    conv10 = Conv3D(128, conv_size, padding='same', name = 'decoder_stage3b_conv')(conv10)\n",
    "    conv10 = BatchNormalization(name = 'decoder_stage3b_bn')(conv10)   #Not in the original network. \n",
    "    conv10 = Activation(\"relu\", name = 'decoder_stage3b_relu')(conv10)\n",
    "    drop10 = Dropout(dropout_rate_1, name = 'block10_drop')(conv10)\n",
    "\n",
    "    up11 = Conv3DTranspose(128, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop10)\n",
    "    #up11 = Conv3DTranspose UpSampling3D(size = (2, 2, 2), name = 'decoder_stage4_upsampling')(conv10)\n",
    "    merge11 = concatenate([up11, drop1], name = 'decoder_stage4_concat', axis=4)\n",
    "    \n",
    "    conv11 = Conv3D(64, conv_size, padding='same', name='decoder_stage4a_conv')(merge11)\n",
    "    conv11 = BatchNormalization(name='decoder_stage4a_bn')(conv11)   #Not in the original network. \n",
    "    conv11 = Activation(\"relu\", name='decoder_stage4a_relu')(conv11)\n",
    "    conv11 = Conv3D(64, conv_size, padding='same', name='decoder_stage4b_conv')(conv11)\n",
    "    conv11 = BatchNormalization(name='decoder_stage4b_bn')(conv11)   #Not in the original network. \n",
    "    conv11 = Activation(\"relu\", name='decoder_stage4b_relu')(conv11)\n",
    "    drop11 = Dropout(dropout_rate_1, name = 'block11_drop')(conv11)\n",
    "\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv3D(n_classes, 1, padding=\"same\", activation=activation, name='final_conv')(drop11)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs=x, outputs=outputs, name=\"3D-UNet\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "de39c646-f2ae-4dec-b4f6-53e0fd4a91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv3D, MaxPooling3D, UpSampling3D, concatenate, Conv3DTranspose, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation\n",
    "\n",
    "\n",
    "\n",
    "def Unet3D(input_shape, n_classes, conv_size=(4, 4, 4), dropout_rate_1=0.1,dropout_rate_2=0.2,dropout_rate_3=0.3,dropout_rate_4=0.5):\n",
    "    x = Input(input_shape)\n",
    "\n",
    "    conv1 = Conv3D(64, conv_size, padding='same', name = 'block1_conv1')(x)\n",
    "    conv1 = BatchNormalization(name = 'block1_bn1')(conv1)   #Not in the original network. \n",
    "    conv1 = Activation(\"relu\", name = 'block1_relu1')(conv1)\n",
    "    \n",
    "    conv1 = Conv3D(64, conv_size, padding='same', name = 'block1_conv2')(conv1)\n",
    "    conv1 = BatchNormalization(name = 'block1_bn2')(conv1)   #Not in the original network. \n",
    "    conv1 = Activation(\"relu\", name = 'block1_relu2')(conv1)\n",
    "    \n",
    "    drop1 = Dropout(dropout_rate_1, name = 'block1_drop')(conv1)\n",
    "    pool1 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block1_pool')(drop1)\n",
    "\n",
    "    conv2 = Conv3D(128, conv_size, padding='same', name = 'block2_conv1')(pool1)\n",
    "    conv2 = BatchNormalization(name = 'block2_bn1')(conv2)   #Not in the original network. \n",
    "    conv2 = Activation(\"relu\",  name = 'block2_relu1')(conv2)\n",
    "    conv2 = Conv3D(128,conv_size, padding='same',  name = 'block2_conv2')(conv2)\n",
    "    conv2 = BatchNormalization( name = 'block2_bn2')(conv2)   #Not in the original network. \n",
    "    conv2 = Activation(\"relu\",  name = 'block2_relu2')(conv2)\n",
    "    drop2 = Dropout(dropout_rate_1, name = 'block2_drop')(conv2)\n",
    "    pool2 = MaxPooling3D(pool_size=(2, 2, 2),  name = 'block2_pool')(drop2)\n",
    "\n",
    "    conv3 = Conv3D(256, conv_size, padding='same', name = 'block3_conv1')(pool2)\n",
    "    conv3 = BatchNormalization(name = 'block3_bn1')(conv3)   #Not in the original network. \n",
    "    conv3 = Activation(\"relu\", name = 'block3_relu1')(conv3)\n",
    "    conv3 = Conv3D(256, conv_size, padding='same', name = 'block3_conv2')(conv3)\n",
    "    conv3 = BatchNormalization( name = 'block3_bn2')(conv3)   #Not in the original network. \n",
    "    conv3 = Activation(\"relu\", name = 'block3_relu2')(conv3)\n",
    "    drop3 = Dropout(dropout_rate_2, name = 'block3_drop')(conv3)\n",
    "    pool3 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block3_pool')(drop3)\n",
    "\n",
    "    conv4 = Conv3D(512, conv_size, padding='same', name = 'block4_conv1')(pool3)\n",
    "    conv4 = BatchNormalization( name = 'block4_bn1')(conv4)   #Not in the original network. \n",
    "    conv4 = Activation(\"relu\", name = 'block4_relu1')(conv4)\n",
    "    conv4 = Conv3D(512, conv_size, padding='same', name = 'block4_conv2')(conv4)\n",
    "    conv4 = BatchNormalization(name = 'block4_bn2')(conv4)   #Not in the original network. \n",
    "    conv4 = Activation(\"relu\", name = 'block4_relu2')(conv4)\n",
    "    drop4 = Dropout(dropout_rate_2, name = 'block4_drop')(conv4)\n",
    "    pool4 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block4_pool')(drop4)\n",
    "\n",
    "    conv5 = Conv3D(512, conv_size, padding='same', name = 'block5_conv1')(pool4)\n",
    "    conv5 = BatchNormalization(name = 'block5_bn1')(conv5)   #Not in the original network. \n",
    "    conv5 = Activation(\"relu\", name = 'block5_relu1')(conv5)\n",
    "    conv5 = Conv3D(512, conv_size, padding='same', name = 'block5_conv2')(conv5)\n",
    "    conv5 = BatchNormalization(name = 'block5_bn2')(conv5)   #Not in the original network. \n",
    "    conv5 = Activation(\"relu\", name = 'block5_relu2')(conv5)\n",
    "    drop5 = Dropout(dropout_rate_3, name = 'block5_drop')(conv5)\n",
    "    pool5 = MaxPooling3D(pool_size=(2, 2, 2), name = 'block5_pool')(drop5)\n",
    "    \n",
    "    # middle\n",
    "    conv6 = Conv3D(512, conv_size, padding='same', name ='center_block1_conv' )(pool5)\n",
    "    conv6 = BatchNormalization(name ='center_block1_bn')(conv6)   #Not in the original network. \n",
    "    conv6 = Activation(\"relu\", name ='center_block1_relu')(conv6)\n",
    "    conv6 = Conv3D(512, conv_size, padding='same', name ='center_block2_conv')(conv6)\n",
    "    conv6 = BatchNormalization(name ='center_block2_bn')(conv6)   #Not in the original network. \n",
    "    conv6 = Activation(\"relu\", name ='center_block2_relu')(conv6)\n",
    "    drop6 = Dropout(dropout_rate_4, name = 'center_block_drop')(conv6)\n",
    "\n",
    "\n",
    "    up7 = Conv3DTranspose(512, (2, 2, 2), strides= (2, 2, 2), padding=\"same\",  name = 'decoder_stage0_upsampling')(drop6)\n",
    "    #up7 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage0_upsampling')(drop6)\n",
    "    merge7 = concatenate([up7, drop5], name='decoder_stage0_concat', axis=4)\n",
    "    \n",
    "    conv7 = Conv3D(256,  conv_size, padding='same', name = 'decoder_stage0a_conv')(merge7)\n",
    "    conv7 = BatchNormalization(name = 'decoder_stage0a_bn')(conv7)   #Not in the original network. \n",
    "    conv7 = Activation(\"relu\", name = 'decoder_stage0a_relu')(conv7)\n",
    "    conv7 = Conv3D(256,  conv_size, padding='same', name = 'decoder_stage0b_conv')(conv7)\n",
    "    conv7 = BatchNormalization(name = 'decoder_stage0b_bn')(conv7)   #Not in the original network. \n",
    "    conv7 = Activation(\"relu\", name = 'decoder_stage0b_relu')(conv7)\n",
    "    drop7 = Dropout(dropout_rate_3, name = 'block7_drop')(conv7)\n",
    "\n",
    "    up8 = Conv3DTranspose(256, (2, 2, 2), strides= (2, 2, 2), padding=\"same\")(drop7)\n",
    "    #up8 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage1_upsampling')(conv7)\n",
    "    merge8 = concatenate([up8, drop4], name='decoder_stage1_concat', axis=4)\n",
    "    \n",
    "    conv8 = Conv3D(128,  conv_size, padding='same', name = 'decoder_stage1a_conv')(merge8)\n",
    "    conv8 = BatchNormalization(name = 'decoder_stage1a_bn')(conv8)   #Not in the original network. \n",
    "    conv8 = Activation(\"relu\", name = 'decoder_stage1a_relu')(conv8)\n",
    "    conv8 = Conv3D(128, conv_size, padding='same', name = 'decoder_stage1b_conv')(conv8)\n",
    "    conv8 = BatchNormalization(name = 'decoder_stage1b_bn')(conv8)   #Not in the original network. \n",
    "    conv8 = Activation(\"relu\", name = 'decoder_stage1b_relu')(conv8)\n",
    "    drop8 = Dropout(dropout_rate_2, name = 'block8_drop')(conv8)\n",
    "    \n",
    "    up9 = Conv3DTranspose(128, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop8)\n",
    "    #up9 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage2_upsampling')(conv8)\n",
    "    merge9 = concatenate([up9, drop3], name='decoder_stage2_concat', axis=4)\n",
    "\n",
    "    conv9 = Conv3D(64,  conv_size, padding='same', name = 'decoder_stage2a_conv')(merge9)\n",
    "    conv9 = BatchNormalization(name = 'decoder_stage2a_bn')(conv9)   #Not in the original network. \n",
    "    conv9 = Activation(\"relu\", name = 'decoder_stage2a_relu')(conv9)\n",
    "    conv9 = Conv3D(64,  conv_size, padding='same', name = 'decoder_stage2b_conv')(conv9)\n",
    "    conv9 = BatchNormalization(name = 'decoder_stage2b_bn')(conv9)   #Not in the original network. \n",
    "    conv9 = Activation(\"relu\", name = 'decoder_stage2b_relu')(conv9)\n",
    "    drop9 = Dropout(dropout_rate_2, name = 'block9_drop')(conv9)\n",
    "\n",
    "    up10 = Conv3DTranspose(64, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop9)\n",
    "    #up10 = UpSampling3D(size = (2, 2, 2), name = 'decoder_stage3_upsampling')(conv9)\n",
    "    merge10 = concatenate([up10, drop2], name='decoder_stage3_concat', axis=4)\n",
    "\n",
    "    conv10 = Conv3D(32,conv_size, padding='same', name = 'decoder_stage3a_conv')(merge10)\n",
    "    conv10 = BatchNormalization(name = 'decoder_stage3a_bn')(conv10)   #Not in the original network. \n",
    "    conv10 = Activation(\"relu\", name = 'decoder_stage3a_relu')(conv10)\n",
    "    conv10 = Conv3D(32, conv_size, padding='same', name = 'decoder_stage3b_conv')(conv10)\n",
    "    conv10 = BatchNormalization(name = 'decoder_stage3b_bn')(conv10)   #Not in the original network. \n",
    "    conv10 = Activation(\"relu\", name = 'decoder_stage3b_relu')(conv10)\n",
    "    drop10 = Dropout(dropout_rate_1, name = 'block10_drop')(conv10)\n",
    "\n",
    "    up11 = Conv3DTranspose(32, (2, 2, 2), strides=  (2, 2, 2), padding=\"same\")(drop10)\n",
    "    #up11 = Conv3DTranspose UpSampling3D(size = (2, 2, 2), name = 'decoder_stage4_upsampling')(conv10)\n",
    "    merge11 = concatenate([up11, drop1], name = 'decoder_stage4_concat', axis=4)\n",
    "    \n",
    "    conv11 = Conv3D(16, conv_size, padding='same', name='decoder_stage4a_conv')(merge11)\n",
    "    conv11 = BatchNormalization(name='decoder_stage4a_bn')(conv11)   #Not in the original network. \n",
    "    conv11 = Activation(\"relu\", name='decoder_stage4a_relu')(conv11)\n",
    "    conv11 = Conv3D(16, conv_size, padding='same', name='decoder_stage4b_conv')(conv11)\n",
    "    conv11 = BatchNormalization(name='decoder_stage4b_bn')(conv11)   #Not in the original network. \n",
    "    conv11 = Activation(\"relu\", name='decoder_stage4b_relu')(conv11)\n",
    "    drop11 = Dropout(dropout_rate_1, name = 'block11_drop')(conv11)\n",
    "    if n_classes == 1:  #Binary\n",
    "      activation = 'sigmoid'\n",
    "    else:\n",
    "      activation = 'softmax'\n",
    "\n",
    "    outputs = Conv3D(n_classes, 1, padding=\"same\", activation=activation, name='final_conv')(drop11)  #Change the activation based on n_classes\n",
    "    print(activation)\n",
    "\n",
    "    model = Model(inputs=x, outputs=outputs, name=\"3D-UNet\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e14d16b2-f962-4944-a511-703049874af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('./X_train.npy')\n",
    "X_test = np.load('./X_test.npy')\n",
    "y_train = np.load('./y_train.npy')\n",
    "y_test = np.load('./y_test.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f09d5cb-6e68-4398-895d-0915b764537a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(604, 32, 64, 64, 3) (604, 32, 64, 64, 3)\n",
      "140\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Sanity check, view few mages\n",
    "import random\n",
    "import numpy as np\n",
    "print(X_train_prep.shape, y_train.shape)\n",
    "image_number = random.randint(0, len(X_train_prep))\n",
    "print(image_number)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_train_prep[image_number,:,:,28,1])\n",
    "plt.subplot(122)\n",
    "plt.imshow(y_train[image_number,:,:,28,1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efedd4eb-b0a0-4694-b9b3-a633871b65f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 0.0001\n",
    "optim = tf.keras.optimizers.Adam(LR)\n",
    "dice_loss = sm.losses.DiceLoss(class_weights=np.array([0.333, 0.333, 0.333])) #优点：对于不平衡的数据集和像素级别的分割任务效果较好，能够更好地处理类别间的不平衡。缺点：对噪声敏感，容易受到边缘效应的影响。\n",
    "focal_loss = sm.losses.CategoricalFocalLoss()#优点：能够解决类别不平衡问题，通过调节alpha和gamma参数可以进一步调整损失函数的重点。缺点：需要调节额外的参数，可能需要进行一定的调参工作。\n",
    "total_loss = dice_loss + (1 * focal_loss)\n",
    "metrics = [sm.metrics.IOUScore(threshold=0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3895e396-6b26-4dfc-ae5f-f994cadf0839",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax\n",
      "Model: \"3D-UNet\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_13 (InputLayer)          [(None, 32, 64, 64,  0           []                               \n",
      "                                 3)]                                                              \n",
      "                                                                                                  \n",
      " block1_conv1 (Conv3D)          (None, 32, 64, 64,   12352       ['input_13[0][0]']               \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block1_bn1 (BatchNormalization  (None, 32, 64, 64,   256        ['block1_conv1[0][0]']           \n",
      " )                              64)                                                               \n",
      "                                                                                                  \n",
      " block1_relu1 (Activation)      (None, 32, 64, 64,   0           ['block1_bn1[0][0]']             \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block1_conv2 (Conv3D)          (None, 32, 64, 64,   262208      ['block1_relu1[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block1_bn2 (BatchNormalization  (None, 32, 64, 64,   256        ['block1_conv2[0][0]']           \n",
      " )                              64)                                                               \n",
      "                                                                                                  \n",
      " block1_relu2 (Activation)      (None, 32, 64, 64,   0           ['block1_bn2[0][0]']             \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block1_drop (Dropout)          (None, 32, 64, 64,   0           ['block1_relu2[0][0]']           \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block1_pool (MaxPooling3D)     (None, 16, 32, 32,   0           ['block1_drop[0][0]']            \n",
      "                                64)                                                               \n",
      "                                                                                                  \n",
      " block2_conv1 (Conv3D)          (None, 16, 32, 32,   524416      ['block1_pool[0][0]']            \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " block2_bn1 (BatchNormalization  (None, 16, 32, 32,   512        ['block2_conv1[0][0]']           \n",
      " )                              128)                                                              \n",
      "                                                                                                  \n",
      " block2_relu1 (Activation)      (None, 16, 32, 32,   0           ['block2_bn1[0][0]']             \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " block2_conv2 (Conv3D)          (None, 16, 32, 32,   1048704     ['block2_relu1[0][0]']           \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " block2_bn2 (BatchNormalization  (None, 16, 32, 32,   512        ['block2_conv2[0][0]']           \n",
      " )                              128)                                                              \n",
      "                                                                                                  \n",
      " block2_relu2 (Activation)      (None, 16, 32, 32,   0           ['block2_bn2[0][0]']             \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " block2_drop (Dropout)          (None, 16, 32, 32,   0           ['block2_relu2[0][0]']           \n",
      "                                128)                                                              \n",
      "                                                                                                  \n",
      " block2_pool (MaxPooling3D)     (None, 8, 16, 16, 1  0           ['block2_drop[0][0]']            \n",
      "                                28)                                                               \n",
      "                                                                                                  \n",
      " block3_conv1 (Conv3D)          (None, 8, 16, 16, 2  2097408     ['block2_pool[0][0]']            \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " block3_bn1 (BatchNormalization  (None, 8, 16, 16, 2  1024       ['block3_conv1[0][0]']           \n",
      " )                              56)                                                               \n",
      "                                                                                                  \n",
      " block3_relu1 (Activation)      (None, 8, 16, 16, 2  0           ['block3_bn1[0][0]']             \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " block3_conv2 (Conv3D)          (None, 8, 16, 16, 2  4194560     ['block3_relu1[0][0]']           \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " block3_bn2 (BatchNormalization  (None, 8, 16, 16, 2  1024       ['block3_conv2[0][0]']           \n",
      " )                              56)                                                               \n",
      "                                                                                                  \n",
      " block3_relu2 (Activation)      (None, 8, 16, 16, 2  0           ['block3_bn2[0][0]']             \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " block3_drop (Dropout)          (None, 8, 16, 16, 2  0           ['block3_relu2[0][0]']           \n",
      "                                56)                                                               \n",
      "                                                                                                  \n",
      " block3_pool (MaxPooling3D)     (None, 4, 8, 8, 256  0           ['block3_drop[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_conv1 (Conv3D)          (None, 4, 8, 8, 512  8389120     ['block3_pool[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_bn1 (BatchNormalization  (None, 4, 8, 8, 512  2048       ['block4_conv1[0][0]']           \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block4_relu1 (Activation)      (None, 4, 8, 8, 512  0           ['block4_bn1[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_conv2 (Conv3D)          (None, 4, 8, 8, 512  16777728    ['block4_relu1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_bn2 (BatchNormalization  (None, 4, 8, 8, 512  2048       ['block4_conv2[0][0]']           \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block4_relu2 (Activation)      (None, 4, 8, 8, 512  0           ['block4_bn2[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_drop (Dropout)          (None, 4, 8, 8, 512  0           ['block4_relu2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block4_pool (MaxPooling3D)     (None, 2, 4, 4, 512  0           ['block4_drop[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_conv1 (Conv3D)          (None, 2, 4, 4, 512  16777728    ['block4_pool[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_bn1 (BatchNormalization  (None, 2, 4, 4, 512  2048       ['block5_conv1[0][0]']           \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block5_relu1 (Activation)      (None, 2, 4, 4, 512  0           ['block5_bn1[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_conv2 (Conv3D)          (None, 2, 4, 4, 512  16777728    ['block5_relu1[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_bn2 (BatchNormalization  (None, 2, 4, 4, 512  2048       ['block5_conv2[0][0]']           \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " block5_relu2 (Activation)      (None, 2, 4, 4, 512  0           ['block5_bn2[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_drop (Dropout)          (None, 2, 4, 4, 512  0           ['block5_relu2[0][0]']           \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block5_pool (MaxPooling3D)     (None, 1, 2, 2, 512  0           ['block5_drop[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " center_block1_conv (Conv3D)    (None, 1, 2, 2, 512  16777728    ['block5_pool[0][0]']            \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " center_block1_bn (BatchNormali  (None, 1, 2, 2, 512  2048       ['center_block1_conv[0][0]']     \n",
      " zation)                        )                                                                 \n",
      "                                                                                                  \n",
      " center_block1_relu (Activation  (None, 1, 2, 2, 512  0          ['center_block1_bn[0][0]']       \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " center_block2_conv (Conv3D)    (None, 1, 2, 2, 512  16777728    ['center_block1_relu[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " center_block2_bn (BatchNormali  (None, 1, 2, 2, 512  2048       ['center_block2_conv[0][0]']     \n",
      " zation)                        )                                                                 \n",
      "                                                                                                  \n",
      " center_block2_relu (Activation  (None, 1, 2, 2, 512  0          ['center_block2_bn[0][0]']       \n",
      " )                              )                                                                 \n",
      "                                                                                                  \n",
      " center_block_drop (Dropout)    (None, 1, 2, 2, 512  0           ['center_block2_relu[0][0]']     \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0_upsampling (Con  (None, 2, 4, 4, 512  2097664    ['center_block_drop[0][0]']      \n",
      " v3DTranspose)                  )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0_concat (Concate  (None, 2, 4, 4, 102  0          ['decoder_stage0_upsampling[0][0]\n",
      " nate)                          4)                               ',                               \n",
      "                                                                  'block5_drop[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_stage0a_conv (Conv3D)  (None, 2, 4, 4, 256  16777472    ['decoder_stage0_concat[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0a_bn (BatchNorma  (None, 2, 4, 4, 256  1024       ['decoder_stage0a_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0a_relu (Activati  (None, 2, 4, 4, 256  0          ['decoder_stage0a_bn[0][0]']     \n",
      " on)                            )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0b_conv (Conv3D)  (None, 2, 4, 4, 256  4194560     ['decoder_stage0a_relu[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0b_bn (BatchNorma  (None, 2, 4, 4, 256  1024       ['decoder_stage0b_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage0b_relu (Activati  (None, 2, 4, 4, 256  0          ['decoder_stage0b_bn[0][0]']     \n",
      " on)                            )                                                                 \n",
      "                                                                                                  \n",
      " block7_drop (Dropout)          (None, 2, 4, 4, 256  0           ['decoder_stage0b_relu[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose_32 (Conv3DTra  (None, 4, 8, 8, 256  524544     ['block7_drop[0][0]']            \n",
      " nspose)                        )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1_concat (Concate  (None, 4, 8, 8, 768  0          ['conv3d_transpose_32[0][0]',    \n",
      " nate)                          )                                 'block4_drop[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_stage1a_conv (Conv3D)  (None, 4, 8, 8, 128  6291584     ['decoder_stage1_concat[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1a_bn (BatchNorma  (None, 4, 8, 8, 128  512        ['decoder_stage1a_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1a_relu (Activati  (None, 4, 8, 8, 128  0          ['decoder_stage1a_bn[0][0]']     \n",
      " on)                            )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1b_conv (Conv3D)  (None, 4, 8, 8, 128  1048704     ['decoder_stage1a_relu[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1b_bn (BatchNorma  (None, 4, 8, 8, 128  512        ['decoder_stage1b_conv[0][0]']   \n",
      " lization)                      )                                                                 \n",
      "                                                                                                  \n",
      " decoder_stage1b_relu (Activati  (None, 4, 8, 8, 128  0          ['decoder_stage1b_bn[0][0]']     \n",
      " on)                            )                                                                 \n",
      "                                                                                                  \n",
      " block8_drop (Dropout)          (None, 4, 8, 8, 128  0           ['decoder_stage1b_relu[0][0]']   \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv3d_transpose_33 (Conv3DTra  (None, 8, 16, 16, 1  131200     ['block8_drop[0][0]']            \n",
      " nspose)                        28)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage2_concat (Concate  (None, 8, 16, 16, 3  0          ['conv3d_transpose_33[0][0]',    \n",
      " nate)                          84)                               'block3_drop[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_stage2a_conv (Conv3D)  (None, 8, 16, 16, 6  1572928     ['decoder_stage2_concat[0][0]']  \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " decoder_stage2a_bn (BatchNorma  (None, 8, 16, 16, 6  256        ['decoder_stage2a_conv[0][0]']   \n",
      " lization)                      4)                                                                \n",
      "                                                                                                  \n",
      " decoder_stage2a_relu (Activati  (None, 8, 16, 16, 6  0          ['decoder_stage2a_bn[0][0]']     \n",
      " on)                            4)                                                                \n",
      "                                                                                                  \n",
      " decoder_stage2b_conv (Conv3D)  (None, 8, 16, 16, 6  262208      ['decoder_stage2a_relu[0][0]']   \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " decoder_stage2b_bn (BatchNorma  (None, 8, 16, 16, 6  256        ['decoder_stage2b_conv[0][0]']   \n",
      " lization)                      4)                                                                \n",
      "                                                                                                  \n",
      " decoder_stage2b_relu (Activati  (None, 8, 16, 16, 6  0          ['decoder_stage2b_bn[0][0]']     \n",
      " on)                            4)                                                                \n",
      "                                                                                                  \n",
      " block9_drop (Dropout)          (None, 8, 16, 16, 6  0           ['decoder_stage2b_relu[0][0]']   \n",
      "                                4)                                                                \n",
      "                                                                                                  \n",
      " conv3d_transpose_34 (Conv3DTra  (None, 16, 32, 32,   32832      ['block9_drop[0][0]']            \n",
      " nspose)                        64)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3_concat (Concate  (None, 16, 32, 32,   0          ['conv3d_transpose_34[0][0]',    \n",
      " nate)                          192)                              'block2_drop[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_stage3a_conv (Conv3D)  (None, 16, 32, 32,   393248      ['decoder_stage3_concat[0][0]']  \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3a_bn (BatchNorma  (None, 16, 32, 32,   128        ['decoder_stage3a_conv[0][0]']   \n",
      " lization)                      32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3a_relu (Activati  (None, 16, 32, 32,   0          ['decoder_stage3a_bn[0][0]']     \n",
      " on)                            32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3b_conv (Conv3D)  (None, 16, 32, 32,   65568       ['decoder_stage3a_relu[0][0]']   \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3b_bn (BatchNorma  (None, 16, 32, 32,   128        ['decoder_stage3b_conv[0][0]']   \n",
      " lization)                      32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage3b_relu (Activati  (None, 16, 32, 32,   0          ['decoder_stage3b_bn[0][0]']     \n",
      " on)                            32)                                                               \n",
      "                                                                                                  \n",
      " block10_drop (Dropout)         (None, 16, 32, 32,   0           ['decoder_stage3b_relu[0][0]']   \n",
      "                                32)                                                               \n",
      "                                                                                                  \n",
      " conv3d_transpose_35 (Conv3DTra  (None, 32, 64, 64,   8224       ['block10_drop[0][0]']           \n",
      " nspose)                        32)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4_concat (Concate  (None, 32, 64, 64,   0          ['conv3d_transpose_35[0][0]',    \n",
      " nate)                          96)                               'block1_drop[0][0]']            \n",
      "                                                                                                  \n",
      " decoder_stage4a_conv (Conv3D)  (None, 32, 64, 64,   98320       ['decoder_stage4_concat[0][0]']  \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4a_bn (BatchNorma  (None, 32, 64, 64,   64         ['decoder_stage4a_conv[0][0]']   \n",
      " lization)                      16)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4a_relu (Activati  (None, 32, 64, 64,   0          ['decoder_stage4a_bn[0][0]']     \n",
      " on)                            16)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4b_conv (Conv3D)  (None, 32, 64, 64,   16400       ['decoder_stage4a_relu[0][0]']   \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4b_bn (BatchNorma  (None, 32, 64, 64,   64         ['decoder_stage4b_conv[0][0]']   \n",
      " lization)                      16)                                                               \n",
      "                                                                                                  \n",
      " decoder_stage4b_relu (Activati  (None, 32, 64, 64,   0          ['decoder_stage4b_bn[0][0]']     \n",
      " on)                            16)                                                               \n",
      "                                                                                                  \n",
      " block11_drop (Dropout)         (None, 32, 64, 64,   0           ['decoder_stage4b_relu[0][0]']   \n",
      "                                16)                                                               \n",
      "                                                                                                  \n",
      " final_conv (Conv3D)            (None, 32, 64, 64,   51          ['block11_drop[0][0]']           \n",
      "                                3)                                                                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 133,952,755\n",
      "Trainable params: 133,942,835\n",
      "Non-trainable params: 9,920\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Unet3D(( 32, 64, 64, 3), n_classes=3)\n",
    "model.compile(optimizer = optim, loss=total_loss, metrics=metrics)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c1e00b51-a0b1-49b7-bbd7-64979bfe7198",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "#early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights = False)\n",
    "model_checkpoint = ModelCheckpoint('./best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "callbacks=[#early_stopping,\n",
    "           model_checkpoint,\n",
    "                  CSVLogger('./history_au.csv'),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=10)\n",
    "          ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2b9a0266-2e64-4aab-83e2-305fa9c82e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "302/302 [==============================] - 77s 237ms/step - loss: 0.8909 - iou_score: 0.5000 - val_loss: 0.8880 - val_iou_score: 0.5055 - lr: 1.0000e-04\n",
      "Epoch 2/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.8634 - iou_score: 0.5570 - val_loss: 0.8636 - val_iou_score: 0.5170 - lr: 1.0000e-04\n",
      "Epoch 3/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.8445 - iou_score: 0.5686 - val_loss: 0.8445 - val_iou_score: 0.5494 - lr: 1.0000e-04\n",
      "Epoch 4/150\n",
      "302/302 [==============================] - 72s 238ms/step - loss: 0.8273 - iou_score: 0.5822 - val_loss: 0.8342 - val_iou_score: 0.5345 - lr: 1.0000e-04\n",
      "Epoch 5/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.8143 - iou_score: 0.5883 - val_loss: 0.8244 - val_iou_score: 0.5394 - lr: 1.0000e-04\n",
      "Epoch 6/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.8029 - iou_score: 0.5986 - val_loss: 0.8254 - val_iou_score: 0.5203 - lr: 1.0000e-04\n",
      "Epoch 7/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7938 - iou_score: 0.6076 - val_loss: 0.8056 - val_iou_score: 0.5611 - lr: 1.0000e-04\n",
      "Epoch 8/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7876 - iou_score: 0.6124 - val_loss: 0.8056 - val_iou_score: 0.5544 - lr: 1.0000e-04\n",
      "Epoch 9/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7817 - iou_score: 0.6227 - val_loss: 0.8065 - val_iou_score: 0.5491 - lr: 1.0000e-04\n",
      "Epoch 10/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7768 - iou_score: 0.6300 - val_loss: 0.7983 - val_iou_score: 0.5676 - lr: 1.0000e-04\n",
      "Epoch 11/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7734 - iou_score: 0.6353 - val_loss: 0.7982 - val_iou_score: 0.5609 - lr: 1.0000e-04\n",
      "Epoch 12/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7697 - iou_score: 0.6432 - val_loss: 0.8009 - val_iou_score: 0.5582 - lr: 1.0000e-04\n",
      "Epoch 13/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7683 - iou_score: 0.6451 - val_loss: 0.8010 - val_iou_score: 0.5572 - lr: 1.0000e-04\n",
      "Epoch 14/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7631 - iou_score: 0.6578 - val_loss: 0.7962 - val_iou_score: 0.5663 - lr: 1.0000e-04\n",
      "Epoch 15/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7631 - iou_score: 0.6567 - val_loss: 0.7966 - val_iou_score: 0.5674 - lr: 1.0000e-04\n",
      "Epoch 16/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7593 - iou_score: 0.6674 - val_loss: 0.7953 - val_iou_score: 0.5701 - lr: 1.0000e-04\n",
      "Epoch 17/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7583 - iou_score: 0.6687 - val_loss: 0.8107 - val_iou_score: 0.5439 - lr: 1.0000e-04\n",
      "Epoch 18/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7554 - iou_score: 0.6768 - val_loss: 0.8043 - val_iou_score: 0.5538 - lr: 1.0000e-04\n",
      "Epoch 19/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7531 - iou_score: 0.6825 - val_loss: 0.7972 - val_iou_score: 0.5670 - lr: 1.0000e-04\n",
      "Epoch 20/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7510 - iou_score: 0.6886 - val_loss: 0.7978 - val_iou_score: 0.5668 - lr: 1.0000e-04\n",
      "Epoch 21/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7498 - iou_score: 0.6915 - val_loss: 0.8010 - val_iou_score: 0.5622 - lr: 1.0000e-04\n",
      "Epoch 22/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7468 - iou_score: 0.7004 - val_loss: 0.8004 - val_iou_score: 0.5625 - lr: 1.0000e-04\n",
      "Epoch 23/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7456 - iou_score: 0.7036 - val_loss: 0.7966 - val_iou_score: 0.5677 - lr: 1.0000e-04\n",
      "Epoch 24/150\n",
      "302/302 [==============================] - 72s 238ms/step - loss: 0.7432 - iou_score: 0.7106 - val_loss: 0.7948 - val_iou_score: 0.5733 - lr: 1.0000e-04\n",
      "Epoch 25/150\n",
      "302/302 [==============================] - 72s 237ms/step - loss: 0.7447 - iou_score: 0.7057 - val_loss: 0.7969 - val_iou_score: 0.5710 - lr: 1.0000e-04\n",
      "Epoch 26/150\n",
      "302/302 [==============================] - 71s 236ms/step - loss: 0.7424 - iou_score: 0.7130 - val_loss: 0.7940 - val_iou_score: 0.5746 - lr: 1.0000e-04\n",
      "Epoch 27/150\n",
      "302/302 [==============================] - 73s 241ms/step - loss: 0.7393 - iou_score: 0.7225 - val_loss: 0.7935 - val_iou_score: 0.5785 - lr: 1.0000e-04\n",
      "Epoch 28/150\n",
      "302/302 [==============================] - 72s 236ms/step - loss: 0.7400 - iou_score: 0.7196 - val_loss: 0.7955 - val_iou_score: 0.5702 - lr: 1.0000e-04\n",
      "Epoch 29/150\n",
      "302/302 [==============================] - 71s 235ms/step - loss: 0.7383 - iou_score: 0.7261 - val_loss: 0.7900 - val_iou_score: 0.5841 - lr: 1.0000e-04\n",
      "Epoch 30/150\n",
      "302/302 [==============================] - 72s 237ms/step - loss: 0.7366 - iou_score: 0.7301 - val_loss: 0.7928 - val_iou_score: 0.5791 - lr: 1.0000e-04\n",
      "Epoch 31/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7349 - iou_score: 0.7359 - val_loss: 0.7970 - val_iou_score: 0.5702 - lr: 1.0000e-04\n",
      "Epoch 32/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7329 - iou_score: 0.7423 - val_loss: 0.7984 - val_iou_score: 0.5681 - lr: 1.0000e-04\n",
      "Epoch 33/150\n",
      "302/302 [==============================] - 71s 234ms/step - loss: 0.7329 - iou_score: 0.7418 - val_loss: 0.7863 - val_iou_score: 0.5943 - lr: 1.0000e-04\n",
      "Epoch 34/150\n",
      "302/302 [==============================] - 72s 236ms/step - loss: 0.7317 - iou_score: 0.7460 - val_loss: 0.7952 - val_iou_score: 0.5777 - lr: 1.0000e-04\n",
      "Epoch 35/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7311 - iou_score: 0.7471 - val_loss: 0.7951 - val_iou_score: 0.5754 - lr: 1.0000e-04\n",
      "Epoch 36/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7281 - iou_score: 0.7570 - val_loss: 0.7947 - val_iou_score: 0.5779 - lr: 1.0000e-04\n",
      "Epoch 37/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7291 - iou_score: 0.7540 - val_loss: 0.7950 - val_iou_score: 0.5752 - lr: 1.0000e-04\n",
      "Epoch 38/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7301 - iou_score: 0.7506 - val_loss: 0.7889 - val_iou_score: 0.5902 - lr: 1.0000e-04\n",
      "Epoch 39/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7254 - iou_score: 0.7661 - val_loss: 0.7914 - val_iou_score: 0.5853 - lr: 1.0000e-04\n",
      "Epoch 40/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7253 - iou_score: 0.7667 - val_loss: 0.7917 - val_iou_score: 0.5830 - lr: 1.0000e-04\n",
      "Epoch 41/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7269 - iou_score: 0.7614 - val_loss: 0.7922 - val_iou_score: 0.5815 - lr: 1.0000e-04\n",
      "Epoch 42/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7251 - iou_score: 0.7669 - val_loss: 0.7967 - val_iou_score: 0.5753 - lr: 1.0000e-04\n",
      "Epoch 43/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7244 - iou_score: 0.7692 - val_loss: 0.7920 - val_iou_score: 0.5851 - lr: 1.0000e-04\n",
      "Epoch 44/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7171 - iou_score: 0.7947 - val_loss: 0.7893 - val_iou_score: 0.5908 - lr: 1.0000e-05\n",
      "Epoch 45/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7166 - iou_score: 0.7964 - val_loss: 0.7882 - val_iou_score: 0.5934 - lr: 1.0000e-05\n",
      "Epoch 46/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7158 - iou_score: 0.7996 - val_loss: 0.7888 - val_iou_score: 0.5915 - lr: 1.0000e-05\n",
      "Epoch 47/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7152 - iou_score: 0.8018 - val_loss: 0.7892 - val_iou_score: 0.5913 - lr: 1.0000e-05\n",
      "Epoch 48/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7162 - iou_score: 0.7986 - val_loss: 0.7889 - val_iou_score: 0.5925 - lr: 1.0000e-05\n",
      "Epoch 49/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7149 - iou_score: 0.8027 - val_loss: 0.7892 - val_iou_score: 0.5918 - lr: 1.0000e-05\n",
      "Epoch 50/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7148 - iou_score: 0.8030 - val_loss: 0.7897 - val_iou_score: 0.5910 - lr: 1.0000e-05\n",
      "Epoch 51/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7146 - iou_score: 0.8037 - val_loss: 0.7899 - val_iou_score: 0.5905 - lr: 1.0000e-05\n",
      "Epoch 52/150\n",
      "302/302 [==============================] - 71s 235ms/step - loss: 0.7135 - iou_score: 0.8081 - val_loss: 0.7902 - val_iou_score: 0.5905 - lr: 1.0000e-05\n",
      "Epoch 53/150\n",
      "302/302 [==============================] - 71s 234ms/step - loss: 0.7131 - iou_score: 0.8091 - val_loss: 0.7883 - val_iou_score: 0.5942 - lr: 1.0000e-05\n",
      "Epoch 54/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8111 - val_loss: 0.7890 - val_iou_score: 0.5929 - lr: 1.0000e-06\n",
      "Epoch 55/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7126 - iou_score: 0.8106 - val_loss: 0.7890 - val_iou_score: 0.5929 - lr: 1.0000e-06\n",
      "Epoch 56/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7129 - iou_score: 0.8099 - val_loss: 0.7889 - val_iou_score: 0.5931 - lr: 1.0000e-06\n",
      "Epoch 57/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7123 - iou_score: 0.8123 - val_loss: 0.7892 - val_iou_score: 0.5926 - lr: 1.0000e-06\n",
      "Epoch 58/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7127 - iou_score: 0.8106 - val_loss: 0.7893 - val_iou_score: 0.5923 - lr: 1.0000e-06\n",
      "Epoch 59/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7131 - iou_score: 0.8094 - val_loss: 0.7887 - val_iou_score: 0.5932 - lr: 1.0000e-06\n",
      "Epoch 60/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7125 - iou_score: 0.8114 - val_loss: 0.7893 - val_iou_score: 0.5923 - lr: 1.0000e-06\n",
      "Epoch 61/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7128 - iou_score: 0.8107 - val_loss: 0.7886 - val_iou_score: 0.5934 - lr: 1.0000e-06\n",
      "Epoch 62/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7129 - iou_score: 0.8103 - val_loss: 0.7893 - val_iou_score: 0.5925 - lr: 1.0000e-06\n",
      "Epoch 63/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7125 - iou_score: 0.8116 - val_loss: 0.7896 - val_iou_score: 0.5922 - lr: 1.0000e-06\n",
      "Epoch 64/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7122 - iou_score: 0.8128 - val_loss: 0.7889 - val_iou_score: 0.5928 - lr: 1.0000e-07\n",
      "Epoch 65/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7126 - iou_score: 0.8108 - val_loss: 0.7892 - val_iou_score: 0.5922 - lr: 1.0000e-07\n",
      "Epoch 66/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7130 - iou_score: 0.8099 - val_loss: 0.7896 - val_iou_score: 0.5918 - lr: 1.0000e-07\n",
      "Epoch 67/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7127 - iou_score: 0.8108 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-07\n",
      "Epoch 68/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7133 - iou_score: 0.8084 - val_loss: 0.7895 - val_iou_score: 0.5920 - lr: 1.0000e-07\n",
      "Epoch 69/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7125 - iou_score: 0.8117 - val_loss: 0.7892 - val_iou_score: 0.5924 - lr: 1.0000e-07\n",
      "Epoch 70/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7139 - iou_score: 0.8073 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-07\n",
      "Epoch 71/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7139 - iou_score: 0.8075 - val_loss: 0.7896 - val_iou_score: 0.5922 - lr: 1.0000e-07\n",
      "Epoch 72/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7127 - iou_score: 0.8108 - val_loss: 0.7896 - val_iou_score: 0.5920 - lr: 1.0000e-07\n",
      "Epoch 73/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7129 - iou_score: 0.8102 - val_loss: 0.7895 - val_iou_score: 0.5919 - lr: 1.0000e-07\n",
      "Epoch 74/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7130 - iou_score: 0.8103 - val_loss: 0.7893 - val_iou_score: 0.5924 - lr: 1.0000e-08\n",
      "Epoch 75/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7124 - iou_score: 0.8120 - val_loss: 0.7894 - val_iou_score: 0.5924 - lr: 1.0000e-08\n",
      "Epoch 76/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7134 - iou_score: 0.8087 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-08\n",
      "Epoch 77/150\n",
      "302/302 [==============================] - 70s 232ms/step - loss: 0.7127 - iou_score: 0.8105 - val_loss: 0.7893 - val_iou_score: 0.5925 - lr: 1.0000e-08\n",
      "Epoch 78/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7118 - iou_score: 0.8138 - val_loss: 0.7893 - val_iou_score: 0.5926 - lr: 1.0000e-08\n",
      "Epoch 79/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7130 - iou_score: 0.8101 - val_loss: 0.7895 - val_iou_score: 0.5919 - lr: 1.0000e-08\n",
      "Epoch 80/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7134 - iou_score: 0.8091 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-08\n",
      "Epoch 81/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8116 - val_loss: 0.7894 - val_iou_score: 0.5924 - lr: 1.0000e-08\n",
      "Epoch 82/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7130 - iou_score: 0.8097 - val_loss: 0.7889 - val_iou_score: 0.5927 - lr: 1.0000e-08\n",
      "Epoch 83/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8111 - val_loss: 0.7895 - val_iou_score: 0.5922 - lr: 1.0000e-08\n",
      "Epoch 84/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8113 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-09\n",
      "Epoch 85/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7126 - iou_score: 0.8112 - val_loss: 0.7898 - val_iou_score: 0.5915 - lr: 1.0000e-09\n",
      "Epoch 86/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7130 - iou_score: 0.8106 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-09\n",
      "Epoch 87/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8115 - val_loss: 0.7895 - val_iou_score: 0.5919 - lr: 1.0000e-09\n",
      "Epoch 88/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7132 - iou_score: 0.8096 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-09\n",
      "Epoch 89/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7133 - iou_score: 0.8094 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-09\n",
      "Epoch 90/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7130 - iou_score: 0.8098 - val_loss: 0.7892 - val_iou_score: 0.5925 - lr: 1.0000e-09\n",
      "Epoch 91/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7129 - iou_score: 0.8104 - val_loss: 0.7897 - val_iou_score: 0.5919 - lr: 1.0000e-09\n",
      "Epoch 92/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7127 - iou_score: 0.8107 - val_loss: 0.7891 - val_iou_score: 0.5925 - lr: 1.0000e-09\n",
      "Epoch 93/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8105 - val_loss: 0.7894 - val_iou_score: 0.5924 - lr: 1.0000e-09\n",
      "Epoch 94/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8108 - val_loss: 0.7893 - val_iou_score: 0.5925 - lr: 1.0000e-10\n",
      "Epoch 95/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7135 - iou_score: 0.8083 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-10\n",
      "Epoch 96/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7136 - iou_score: 0.8085 - val_loss: 0.7888 - val_iou_score: 0.5930 - lr: 1.0000e-10\n",
      "Epoch 97/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7138 - iou_score: 0.8077 - val_loss: 0.7889 - val_iou_score: 0.5928 - lr: 1.0000e-10\n",
      "Epoch 98/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8112 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-10\n",
      "Epoch 99/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7132 - iou_score: 0.8099 - val_loss: 0.7893 - val_iou_score: 0.5922 - lr: 1.0000e-10\n",
      "Epoch 100/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7121 - iou_score: 0.8129 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-10\n",
      "Epoch 101/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8109 - val_loss: 0.7896 - val_iou_score: 0.5920 - lr: 1.0000e-10\n",
      "Epoch 102/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8111 - val_loss: 0.7892 - val_iou_score: 0.5926 - lr: 1.0000e-10\n",
      "Epoch 103/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8108 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-10\n",
      "Epoch 104/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7132 - iou_score: 0.8094 - val_loss: 0.7892 - val_iou_score: 0.5923 - lr: 1.0000e-11\n",
      "Epoch 105/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7130 - iou_score: 0.8098 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-11\n",
      "Epoch 106/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7133 - iou_score: 0.8091 - val_loss: 0.7892 - val_iou_score: 0.5923 - lr: 1.0000e-11\n",
      "Epoch 107/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7131 - iou_score: 0.8101 - val_loss: 0.7890 - val_iou_score: 0.5927 - lr: 1.0000e-11\n",
      "Epoch 108/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7125 - iou_score: 0.8115 - val_loss: 0.7893 - val_iou_score: 0.5922 - lr: 1.0000e-11\n",
      "Epoch 109/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7130 - iou_score: 0.8105 - val_loss: 0.7892 - val_iou_score: 0.5922 - lr: 1.0000e-11\n",
      "Epoch 110/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8109 - val_loss: 0.7895 - val_iou_score: 0.5920 - lr: 1.0000e-11\n",
      "Epoch 111/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8100 - val_loss: 0.7891 - val_iou_score: 0.5925 - lr: 1.0000e-11\n",
      "Epoch 112/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7131 - iou_score: 0.8096 - val_loss: 0.7895 - val_iou_score: 0.5923 - lr: 1.0000e-11\n",
      "Epoch 113/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7129 - iou_score: 0.8106 - val_loss: 0.7894 - val_iou_score: 0.5923 - lr: 1.0000e-11\n",
      "Epoch 114/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8107 - val_loss: 0.7892 - val_iou_score: 0.5926 - lr: 1.0000e-12\n",
      "Epoch 115/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7123 - iou_score: 0.8122 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-12\n",
      "Epoch 116/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7125 - iou_score: 0.8113 - val_loss: 0.7893 - val_iou_score: 0.5923 - lr: 1.0000e-12\n",
      "Epoch 117/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7120 - iou_score: 0.8134 - val_loss: 0.7893 - val_iou_score: 0.5923 - lr: 1.0000e-12\n",
      "Epoch 118/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7130 - iou_score: 0.8097 - val_loss: 0.7894 - val_iou_score: 0.5920 - lr: 1.0000e-12\n",
      "Epoch 119/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7133 - iou_score: 0.8096 - val_loss: 0.7890 - val_iou_score: 0.5928 - lr: 1.0000e-12\n",
      "Epoch 120/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7135 - iou_score: 0.8091 - val_loss: 0.7891 - val_iou_score: 0.5928 - lr: 1.0000e-12\n",
      "Epoch 121/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8112 - val_loss: 0.7896 - val_iou_score: 0.5920 - lr: 1.0000e-12\n",
      "Epoch 122/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7122 - iou_score: 0.8122 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-12\n",
      "Epoch 123/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7124 - iou_score: 0.8118 - val_loss: 0.7891 - val_iou_score: 0.5926 - lr: 1.0000e-12\n",
      "Epoch 124/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8111 - val_loss: 0.7893 - val_iou_score: 0.5921 - lr: 1.0000e-13\n",
      "Epoch 125/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7125 - iou_score: 0.8116 - val_loss: 0.7891 - val_iou_score: 0.5925 - lr: 1.0000e-13\n",
      "Epoch 126/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7122 - iou_score: 0.8123 - val_loss: 0.7890 - val_iou_score: 0.5926 - lr: 1.0000e-13\n",
      "Epoch 127/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8105 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-13\n",
      "Epoch 128/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7136 - iou_score: 0.8087 - val_loss: 0.7894 - val_iou_score: 0.5921 - lr: 1.0000e-13\n",
      "Epoch 129/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7125 - iou_score: 0.8116 - val_loss: 0.7890 - val_iou_score: 0.5925 - lr: 1.0000e-13\n",
      "Epoch 130/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8113 - val_loss: 0.7891 - val_iou_score: 0.5925 - lr: 1.0000e-13\n",
      "Epoch 131/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8109 - val_loss: 0.7894 - val_iou_score: 0.5922 - lr: 1.0000e-13\n",
      "Epoch 132/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7131 - iou_score: 0.8097 - val_loss: 0.7892 - val_iou_score: 0.5924 - lr: 1.0000e-13\n",
      "Epoch 133/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8103 - val_loss: 0.7897 - val_iou_score: 0.5916 - lr: 1.0000e-13\n",
      "Epoch 134/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7128 - iou_score: 0.8104 - val_loss: 0.7891 - val_iou_score: 0.5926 - lr: 1.0000e-14\n",
      "Epoch 135/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7134 - iou_score: 0.8084 - val_loss: 0.7890 - val_iou_score: 0.5929 - lr: 1.0000e-14\n",
      "Epoch 136/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8112 - val_loss: 0.7890 - val_iou_score: 0.5928 - lr: 1.0000e-14\n",
      "Epoch 137/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7129 - iou_score: 0.8100 - val_loss: 0.7892 - val_iou_score: 0.5925 - lr: 1.0000e-14\n",
      "Epoch 138/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7124 - iou_score: 0.8119 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-14\n",
      "Epoch 139/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7120 - iou_score: 0.8129 - val_loss: 0.7896 - val_iou_score: 0.5921 - lr: 1.0000e-14\n",
      "Epoch 140/150\n",
      "302/302 [==============================] - 69s 230ms/step - loss: 0.7133 - iou_score: 0.8091 - val_loss: 0.7893 - val_iou_score: 0.5924 - lr: 1.0000e-14\n",
      "Epoch 141/150\n",
      "302/302 [==============================] - 70s 232ms/step - loss: 0.7124 - iou_score: 0.8116 - val_loss: 0.7895 - val_iou_score: 0.5921 - lr: 1.0000e-14\n",
      "Epoch 142/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7119 - iou_score: 0.8134 - val_loss: 0.7894 - val_iou_score: 0.5924 - lr: 1.0000e-14\n",
      "Epoch 143/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7126 - iou_score: 0.8116 - val_loss: 0.7895 - val_iou_score: 0.5922 - lr: 1.0000e-14\n",
      "Epoch 144/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7123 - iou_score: 0.8125 - val_loss: 0.7889 - val_iou_score: 0.5930 - lr: 1.0000e-15\n",
      "Epoch 145/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7132 - iou_score: 0.8094 - val_loss: 0.7898 - val_iou_score: 0.5915 - lr: 1.0000e-15\n",
      "Epoch 146/150\n",
      "302/302 [==============================] - 70s 230ms/step - loss: 0.7134 - iou_score: 0.8091 - val_loss: 0.7893 - val_iou_score: 0.5922 - lr: 1.0000e-15\n",
      "Epoch 147/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7133 - iou_score: 0.8091 - val_loss: 0.7893 - val_iou_score: 0.5922 - lr: 1.0000e-15\n",
      "Epoch 148/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7137 - iou_score: 0.8079 - val_loss: 0.7893 - val_iou_score: 0.5923 - lr: 1.0000e-15\n",
      "Epoch 149/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7123 - iou_score: 0.8125 - val_loss: 0.7895 - val_iou_score: 0.5922 - lr: 1.0000e-15\n",
      "Epoch 150/150\n",
      "302/302 [==============================] - 70s 231ms/step - loss: 0.7127 - iou_score: 0.8110 - val_loss: 0.7892 - val_iou_score: 0.5924 - lr: 1.0000e-15\n"
     ]
    }
   ],
   "source": [
    "#Fit the model\n",
    "history=model.fit(X_train_prep, \n",
    "                  y_train,\n",
    "                  batch_size=2, \n",
    "                  epochs=150,\n",
    "                  validation_data=(X_test_prep,y_test),\n",
    "                  callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_390",
   "language": "python",
   "name": "gpu_390"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
